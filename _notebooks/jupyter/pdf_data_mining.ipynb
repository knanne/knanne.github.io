{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample PDF\n",
    "\n",
    "**Let's take a publicly accessible PDF as a sample, and for fun let's use my Master's thesis.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/knanne/vu_msc_tweetsumm/master/research/KainNanne_MSc_Thesis_ACM.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with urllib.request.urlopen(url) as response, open('sample.pdf', 'wb') as out_file:\n",
    "    shutil.copyfileobj(response, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'sample.pdf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF to Text using pdfminer\n",
    "\n",
    "**Below is a funtion to convert the file to text. Source Credit:** https://stackoverflow.com/a/26495057/5356898"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import StringIO\n",
    "\n",
    "def convert_pdf_to_txt(file):\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "    fp = open(file, 'rb')\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    password = \"\"\n",
    "    maxpages = 0\n",
    "    caching = True\n",
    "    pagenos=set()\n",
    "\n",
    "    for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages, password=password, caching=caching, check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "\n",
    "    text = retstr.getvalue()\n",
    "    \n",
    "    text = text.replace('  ',' ').replace('  ',' ')\n",
    "\n",
    "    fp.close()\n",
    "    device.close()\n",
    "    retstr.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = convert_pdf_to_txt(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now that we have the text of the PDF document as a single string, you may want to apply some fance regular expression to split and parse the text as you wish**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Automating Open Domain Event Summaries by\\n\\nHarnessing Collective Reactions on Twitter\\n\\nKain Nanne\\n\\n∗\\n\\nVU University Amsterdam\\nk.nanne@student.vu.nl\\n\\nABSTRACT\\nMicroblogging sites have become popular platforms for on-\\nline news reporting as well as socially participating in and\\ninteracting with the discussion of real-time events. This pa-\\nper researches an automated solution to the inability of a\\nhuman to wholly consume and comprehend the vast amount\\nof data surrounding topics online. We introduce the Collec-\\ntive Reactions for Event Summarization (CRES) approach,\\nwhich uses an original combination of proven algorithms to\\nharness signals in online activity, social interactions, con-\\ntent metadata, and language overlap to build comprehen-\\nsive summaries of events through collective reactions from\\nthe crowd. The methodology is open sourced as an end-to-\\nend framework exempliﬁed using twelve open domain events.\\nOur experiments consider the two questions of: create a\\nstandard feature set for consistently classifying newsworthi-\\nness in open domain microblog documents, and provide a\\nsummary which improves upon the deﬁned baseline when\\nevaluated using CrowdTruth. Results show promising re-\\nsults towards consistent classiﬁcation on open domain doc-\\numents, and signiﬁcant improvements to our baseline for\\nautomating event summarization on Twitter.\\n\\nKeywords\\ntwitter, automatic summarization, event detection, text min-\\ning, document classiﬁcation\\n\\n∗MSc. Business Information Systems\\n\\n1.\\n\\nINTRODUCTION\\n\\nMicroblogging is the activity of sharing a small amount of\\ninformation over the web. These small documents of infor-\\nmation can include combinations of text and media content,\\nand are typically shared over the public domain. Microblog-\\nging has become increasingly popular for social as well as\\nnews reporting. A 2015 survey by the Pew Research Cen-\\nter found that over 60% of social media users of Facebook\\nand Twitter actively source news from the sites, an increase\\nin over 10% for both sites from 2013. [3] Particularly in\\nnews, given the short time it takes to write and ability to\\nshare with a mass audience instantaneously, it is attractive\\nfor organizations to share smaller pieces of information as\\nreal-time events unfold. As a result of this, an immense\\namount of information surrounding an event is fragmented\\nacross sites and accounts making it impossible for a human\\nto wholly comprehend. As this information is disconnected\\nand drowning in extraneous data, it is a diﬃcult'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf[:2500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF Metadata using pdfminer\n",
    "\n",
    "**You will notice below the metadata in this particular PDF is virtually nonexistent. However, this code is simply a demonstration as to how one would extract such data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdftypes import resolve1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = open(file, 'rb')\n",
    "parser = PDFParser(fp)\n",
    "doc = PDFDocument(parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Author': b'',\n",
       " 'CreationDate': b\"D:20160929165748Z00'00'\",\n",
       " 'Creator': b'LaTeX with hyperref package',\n",
       " 'Keywords': b'',\n",
       " 'ModDate': b\"D:20160929165748Z00'00'\",\n",
       " 'Producer': b'dvips + GPL Ghostscript 9.05',\n",
       " 'Subject': b'',\n",
       " 'Title': b''}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k:v if isinstance(v, bytes) else v.resolve() for k,v in doc.info[0].items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resolve Metadata to XML if exists\n",
    "\n",
    "**Depending on in which system your PDF was created, for example if it was electronically signed in something like Docusign, you may have information on the signers here including emails, names, and dates of form completions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_metadata = doc.catalog['Metadata']\n",
    "resolved_xml = catalog_metadata.resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "try:\n",
    "    soup = BeautifulSoup(resolved_xml.get_data(), 'lxml')\n",
    "except: #PDFNotImplementedError\n",
    "    soup = BeautifulSoup(resolved_xml.rawdata, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xpacket begin='ï»¿' id='W5M0MpCehiHzreSzNTczkc9d'?>\n",
      "<?adobe-xap-filters esc=\"CRLF\"?>\n",
      "<html>\n",
      " <body>\n",
      "  <x:xmpmeta x:xmptk=\"XMP toolkit 2.9.1-13, framework 1.6\" xmlns:x=\"adobe:ns:meta/\">\n",
      "   <rdf:rdf xmlns:ix=\"http://ns.adobe.com/iX/1.0/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
      "    <rdf:description rdf:about=\"uuid:6631a5c2-be82-11f1-0000-c7fc2c5fefc0\" xmlns:pdf=\"http://ns.adobe.com/pdf/1.3/\">\n",
      "     <pdf:producer>\n",
      "      dvips + GPL Ghostscript 9.05\n",
      "     </pdf:producer>\n",
      "     <pdf:keywords>\n",
      "      ()\n",
      "     </pdf:keywords>\n",
      "    </rdf:description>\n",
      "    <rdf:description rdf:about=\"uuid:6631a5c2-be82-11f1-0000-c7fc2c5fefc0\" xmlns:xmp=\"http://ns.adobe.com/xap/1.0/\">\n",
      "     <xmp:modifydate>\n",
      "      2016-09-29T16:57:48Z\n",
      "     </xmp:modifydate>\n",
      "     <xmp:createdate>\n",
      "      2016-09-29T16:57:48Z\n",
      "     </xmp:createdate>\n",
      "     <xmp:creatortool>\n",
      "      LaTeX with hyperref package\n",
      "     </xmp:creatortool>\n",
      "    </rdf:description>\n",
      "    <rdf:description rdf:about=\"uuid:6631a5c2-be82-11f1-0000-c7fc2c5fefc0\" xapmm:documentid=\"uuid:6631a5c2-be82-11f1-0000-c7fc2c5fefc0\" xmlns:xapmm=\"http://ns.adobe.com/xap/1.0/mm/\">\n",
      "    </rdf:description>\n",
      "    <rdf:description dc:format=\"application/pdf\" rdf:about=\"uuid:6631a5c2-be82-11f1-0000-c7fc2c5fefc0\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\">\n",
      "     <dc:title>\n",
      "      <rdf:alt>\n",
      "       <rdf:li xml:lang=\"x-default\">\n",
      "        ()\n",
      "       </rdf:li>\n",
      "      </rdf:alt>\n",
      "     </dc:title>\n",
      "     <dc:creator>\n",
      "      <rdf:seq>\n",
      "       <rdf:li>\n",
      "        ()\n",
      "       </rdf:li>\n",
      "      </rdf:seq>\n",
      "     </dc:creator>\n",
      "     <dc:description>\n",
      "      <rdf:seq>\n",
      "       <rdf:li>\n",
      "        ()\n",
      "       </rdf:li>\n",
      "      </rdf:seq>\n",
      "     </dc:description>\n",
      "    </rdf:description>\n",
      "   </rdf:rdf>\n",
      "  </x:xmpmeta>\n",
      "  <?xpacket end='w'?>\n",
      " </body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "print(soup.prettify(formatter=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You may now want to extract certain data by tags and process as you like**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = soup.find('xmp:createdate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2016-09-29'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.to_datetime(d.text).strftime('%Y-%m-%d')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
