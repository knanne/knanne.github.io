{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression in PySpark\n",
    "\n",
    "This Notebook was originally created in Databricks. You can sign up for the free community edition of Databricks [here](https://community.cloud.databricks.com/) then import this notebook.  \n",
    "\n",
    "This is a very basic introduction on how to build a linear regression model on Spark using Python.  \n",
    "\n",
    "Here are reference docs on Linear Regression in PySpark.  \n",
    "\n",
    "- https://spark.apache.org/docs/latest/mllib-linear-methods.html#regression\n",
    "- https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.regression.LinearRegression\n",
    "- https://docs.databricks.com/spark/latest/mllib/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# generate a random and uniform 2D matrix of correlated data\n",
    "# source: https://stackoverflow.com/a/18684433/5356898\n",
    "\n",
    "xx = np.array([-0.51, 51.2])\n",
    "yy = np.array([0.33, 51.6])\n",
    "means = [xx.mean(), yy.mean()]  \n",
    "stds = [xx.std() / 3, yy.std() / 3]\n",
    "corr = 0.8 # correlation\n",
    "covs = [[stds[0]**2          , stds[0]*stds[1]*corr], \n",
    "        [stds[0]*stds[1]*corr,           stds[1]**2]] \n",
    "\n",
    "data = np.random.multivariate_normal(means, covs, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize(data)\n",
    "rdd2 = rdd1.map(lambda x: [float(i) for i in x])\n",
    "df = rdd2.toDF([\"y\",\"x\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression, LinearRegressionSummary\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"x\"], outputCol=\"features\")\n",
    "\n",
    "lr = LinearRegression(labelCol=\"y\")\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, lr])\n",
    "\n",
    "train, test = df.randomSplit([0.75, 0.25])\n",
    "\n",
    "model = pipeline.fit(train)\n",
    "\n",
    "predictions = model.transform(test)\n",
    "\n",
    "eval = RegressionEvaluator(labelCol=\"y\", predictionCol=\"prediction\")\n",
    "\n",
    "# uncomment below for help\n",
    "#help(eval)\n",
    "#for line in eval.explainParams().split('\\n'):\n",
    "#  print(line)\n",
    "\n",
    "print('RMSE:', eval.evaluate(predictions, {eval.metricName: \"rmse\"}))\n",
    "print('R-squared:', eval.evaluate(predictions, {eval.metricName: \"r2\"}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "name": "test",
  "notebookId": 1970242992222722
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
